{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gc \n",
    "import sys\n",
    "\n",
    "#the basics\n",
    "import pandas as pd, numpy as np, seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# this code is essential for applying progress_apply() in pandas series object\n",
    "tqdm.pandas()\n",
    "\n",
    "#for model evaluation\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera, [6](https://stackoverflow.com/a/1094933/1870254), modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\MINSEOK\\\\Desktop\\\\대학생활\\\\대외활동\\\\kaggle\\\\commonlit-evaluate-student-summaries'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('c:\\\\Users\\\\MINSEOK\\\\Desktop\\\\대학생활\\\\대외활동\\\\kaggle\\\\commonlit-evaluate-student-summaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train=pd.read_csv('prompts_train.csv')\n",
    "prompts_test=pd.read_csv('prompts_test.csv')\n",
    "summaries_train=pd.read_csv('summaries_train.csv')\n",
    "summaries_test=pd.read_csv('summaries_test.csv')\n",
    "\n",
    "sample_submission=pd.read_csv('sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   prompt_id        4 non-null      object\n",
      " 1   prompt_question  4 non-null      object\n",
      " 2   prompt_title     4 non-null      object\n",
      " 3   prompt_text      4 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 256.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "prompts_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b9047</td>\n",
       "      <td>In complete sentences, summarize the structure...</td>\n",
       "      <td>Egyptian Social Structure</td>\n",
       "      <td>Egyptian society was structured like a pyramid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>814d6b</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \\r\\nThe Third Wave experiment took ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_id                                    prompt_question  \\\n",
       "0    39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "1    3b9047  In complete sentences, summarize the structure...   \n",
       "2    814d6b  Summarize how the Third Wave developed over su...   \n",
       "3    ebad26  Summarize the various ways the factory would u...   \n",
       "\n",
       "                prompt_title  \\\n",
       "0                 On Tragedy   \n",
       "1  Egyptian Social Structure   \n",
       "2             The Third Wave   \n",
       "3    Excerpt from The Jungle   \n",
       "\n",
       "                                         prompt_text  \n",
       "0  Chapter 13 \\r\\nAs the sequel to what has alrea...  \n",
       "1  Egyptian society was structured like a pyramid...  \n",
       "2  Background \\r\\nThe Third Wave experiment took ...  \n",
       "3  With one member trimming beef in a cannery, an...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter 13 \\r', 'As the sequel to what has already been said, we must proceed to consider what the poet should aim at, and what he should avoid, in constructing his plots; and by what means the specific effect of Tragedy will be produced. \\r', 'A perfect tragedy should, as we have seen, be arranged not on the simple but on the complex plan. It should, moreover, imitate actions which excite pity and fear, this being the distinctive mark of tragic imitation. It follows plainly, in the first place, that the change of fortune presented must not be the spectacle of a virtuous man brought from prosperity to adversity: for this moves neither pity nor fear; it merely shocks us. Nor, again, that of a bad man passing from adversity to prosperity: for nothing can be more alien to the spirit of Tragedy; it possesses no single tragic quality; it neither satisfies the moral sense nor calls forth pity or fear. Nor, again, should the downfall of the utter villain be exhibited. A plot of this kind would, doubtless, satisfy the moral sense, but it would inspire neither pity nor fear; for pity is aroused by unmerited misfortune, fear by the misfortune of a man like ourselves. Such an event, therefore, will be neither pitiful nor terrible. There remains, then, the character between these two extremes — that of a man who is not eminently good and just, yet whose misfortune is brought about not by vice or depravity, but by some error of judgement or frailty. He must be one who is highly renowned and prosperous — a personage like Oedipus, Thyestes, or other illustrious men of such families. \\r', 'A well-constructed plot should, therefore, be single in its issue, rather than double as some maintain. The change of fortune should be not from bad to good, but, reversely, from good to bad. It should come about as the result not of vice, but of some great error or frailty, in a character either such as we have described, or better rather than worse. The practice of the stage bears out our view. At first the poets recounted any legend that came in their way. Now, the best tragedies are founded on the story of a few houses — on the fortunes of Alcmaeon, Oedipus, Orestes, Meleager, Thyestes, Telephus, and those others who have done or suffered something terrible. A tragedy, then, to be perfect according to the rules of art, should be of this construction. Hence they are in error who censure Euripides just because he follows this principle in his plays, many of which end unhappily. It is, as we have said, the right ending. The best proof is that on the stage and in dramatic competition, such plays, if well worked out, are the most tragic in effect; and Euripides, faulty though he may be in the general management of his subject, yet is felt to be the most tragic of the poets. \\r', 'In the second rank comes the kind of tragedy which some place first. Like the Odyssey, it has a double thread of plot, and also an opposite catastrophe for the good and for the bad. It is accounted the best because of the weakness of the spectators; for the poet is guided in what he writes by the wishes of his audience. The pleasure, however, thence derived is not the true tragic pleasure. It is proper rather to Comedy, where those who, in the piece, are the deadliest enemies — like Orestes and Aegisthus — quit the stage as friends at the close, and no one slays or is slain.']\n",
      "['Egyptian society was structured like a pyramid. At the top were the gods, such as Ra, Osiris, and Isis. Egyptians believed that the gods controlled the universe. Therefore, it was important to keep them happy. They could make the Nile overflow, cause famine, or even bring death. \\r', 'The Egyptians also elevated some human beings to gods. Their leaders, called pharaohs, were believed to be gods in human form. They had absolute power over their subjects. After pharaohs died, huge stone pyramids were built as their tombs. Pharaohs were buried in chambers within the pyramids. \\r', 'Because the people of Egypt believed that their pharaohs were gods, they entrusted their rulers with many responsibilities. Protection was at the top of the list. The pharaoh directed the army in case of a foreign threat or an internal conflict. All laws were enacted at the discretion of the pharaoh. Each farmer paid taxes in the form of grains, which were stored in the pharaoh’s warehouses. This grain was used to feed the people in the event of a famine. \\r', 'The Chain of Command \\r', 'No single person could manage all these duties without assistance. The pharaoh appointed a chief minister called a vizier as a supervisor. The vizier ensured that taxes were collected. \\r', 'Working with the vizier were scribes who kept government records. These high-level employees had mastered a rare skill in ancient Egypt — they could read and write. \\r', 'Noble Aims \\r', 'Right below the pharaoh in status were powerful nobles and priests. Only nobles could hold government posts; in these positions they profited from tributes paid to the pharaoh. Priests were responsible for pleasing the gods. \\r', 'Nobles enjoyed great status and also grew wealthy from donations to the gods. All Egyptians—from pharaohs to farmers—gave gifts to the gods. \\r', 'Soldier On \\r', 'Soldiers fought in wars or quelled domestic uprisings. During long periods of peace, soldiers also supervised the peasants, farmers, and slaves who were involved in building such structures as pyramids and palaces. \\r', 'Skilled workers such as physicians and craftsmen/women made up the middle class. Craftsmen made and sold jewelry, pottery, papyrus products, tools, and other useful things. \\r', 'Naturally, there were people needed to buy goods from artisans and traders. These were the merchants and storekeepers who sold these goods to the public. \\r', 'The Bottom of the Heap \\r', 'At the bottom of the social structure were slaves and farmers. Slavery became the fate of those captured as prisoners of war. In addition to being forced to work on building projects, slaves toiled at the discretion of the pharaoh or nobles. \\r', 'Farmers tended the fields, raised animals, kept canals and reservoirs in good order, worked in the stone quarries, and built the royal monuments. Farmers paid taxes that could amount to as much as 60% of their yearly harvest—that’s a lot of hay! \\r', 'Social mobility was not impossible. A small number of peasants and farmers moved up the economic ladder. Families saved money to send their sons to village schools to learn trades. These schools were run by priests or by artisans. Boys who learned to read and write could become scribes, then go on to gain employment in the government. It was possible for a boy born on a farm to work his way up into the higher ranks of the government. Bureaucracy proved lucrative.']\n",
      "['Background \\r', 'The Third Wave experiment took place at Cubberley High School in Palo Alto, California during the first week of April 1967. History teacher Ron Jones, finding himself unable to explain to his students how people throughout history followed the crowd even when terrible things were happening, decided to demonstrate it to his students through an experiment. Jones announced that he was starting a movement aimed to eliminate democracy. Jones named the movement “The Third Wave” as a symbol of strength, referring to the mythical belief that the third in a series of waves is the strongest. One of the central points of this movement was that democracy’s main weakness is that it favors the individual over the whole community. Jones emphasized this main point of the movement when he created this catchy motto: “Strength through discipline, strength through community, strength through action, strength through pride.” \\r', 'The Experiment \\r', 'Jones started the first day of the experiment emphasizing simple things like proper seating, and drilled the students extensively until they got it right. He then proceeded to enforce strict classroom discipline by emerging as an authoritarian figure. This resulted in dramatic improvements to the efficiency, or orderliness, of the class.  The first day’s session ended with only a few rules. Jones intended it to be a one-day experiment. Students had to be sitting at attention before the second bell, had to stand up to ask or answer questions and had to do it in three words or fewer, and were required to preface each remark with “Mr. Jones.” As the week went on, Jones’ class transformed into a group with a supreme sense of discipline and community. Jones made up a salute resembling that of the Nazi regime and ordered class members to salute each other even outside the class. They all obeyed this command. \\r', 'After only three days, the experiment took on a life of its own, with students from all over the school joining in. The class expanded from initial 30 students to 43 attendees. All of the students showed drastic improvement in their academic skills and tremendous motivation. All of the students were issued a member card and each of them received a special assignment, like designing a Third Wave Banner, stopping non-members from entering the class, or other tasks to bring honor to the movement. Jones instructed the students on how to initiate new members, and by the end of the day the movement had over 200 participants. Jones was surprised that some of the students started reporting to him when other members of the movement failed to abide by the rules. \\r', 'By the fourth day of the experiment, the students became increasingly involved in the project and their discipline and loyalty to the project was so outstanding that Jones felt it was slipping out of control. He decided to terminate the movement, so he lied to students by announcing that the Third Wave was a part of a nationwide movement and that on the next day a presidential candidate of the movement would publicly announce its existence on television. Jones ordered students to attend a noon rally on Friday to witness the announcement. \\r', 'At the end of the week, instead of a televised address of their leader, the students were presented with a blank channel. After a few minutes of waiting, Jones announced that they had been a part of an experiment to demonstrate how people willingly create a sense of superiority over others, and how this can lead people to justify doing horrible things in the name of the state’s honor.']\n",
      "['With one member trimming beef in a cannery, and another working in a sausage factory, the family had a first-hand knowledge of the great majority of Packingtown swindles. For it was the custom, as they found, whenever meat was so spoiled that it could not be used for anything else, either to can it or else to chop it up into sausage. With what had been told them by Jonas, who had worked in the pickle rooms, they could now study the whole of the spoiled-meat industry on the inside, and read a new and grim meaning into that old Packingtown jest—that they use everything of the pig except the squeal. \\r', 'Jonas had told them how the meat that was taken out of pickle would often be found sour, and how they would rub it up with soda to take away the smell, and sell it to be eaten on free-lunch counters; also of all the miracles of chemistry which they performed, giving to any sort of meat, fresh or salted, whole or chopped, any color and any flavor and any odor they chose. In the pickling of hams they had an ingenious apparatus, by which they saved time and increased the capacity of the plant—a machine consisting of a hollow needle attached to a pump; by plunging this needle into the meat and working with his foot, a man could fill a ham with pickle in a few seconds. And yet, in spite of this, there would be hams found spoiled, some of them with an odor so bad that a man could hardly bear to be in the room with them. To pump into these the packers had a second and much stronger pickle which destroyed the odor—a process known to the workers as “giving them thirty per cent.” Also, after the hams had been smoked, there would be found some that had gone to the bad. Formerly these had been sold as “Number Three Grade,” but later on some ingenious person had hit upon a new device, and now they would extract the bone, about which the bad part generally lay, and insert in the hole a white-hot iron. After this invention there was no longer Number One, Two, and Three Grade—there was only Number One Grade. The packers were always originating such schemes—they had what they called “boneless hams,” which were all the odds and ends of pork stuffed into casings; and “California hams,” which were the shoulders, with big knuckle joints, and nearly all the meat cut out; and fancy “skinned hams,” which were made of the oldest hogs, whose skins were so heavy and coarse that no one would buy them—that is, until they had been cooked and chopped fine and labeled “head cheese!” \\r', 'It was only when the whole ham was spoiled that it came into the department of Elzbieta. Cut up by the two-thousand-revolutions- a-minute flyers, and mixed with half a ton of other meat, no odor that ever was in a ham could make any difference. There was never the least attention paid to what was cut up for sausage; there would come all the way back from Europe old sausage that had been rejected, and that was moldy and white – it would be dosed with borax and glycerin, and dumped into the hoppers, and made over again for home consumption. \\r', 'There would be meat that had tumbled out on the floor, in the dirt and sawdust, where the workers had tramped and spit uncounted billions of consumption germs. There would be meat stored in great piles in rooms; and the water from leaky roofs would drip over it, and thousands of rats would race about on it. It was too dark in these storage places to see well, but a man could run his hand over these piles of meat and sweep off handfuls of the dried dung of rats. These rats were nuisances, and the packers would put poisoned bread out for them; they would die, and then rats, bread, and meat would go into the hoppers together. This is no fairy story and no joke; the meat would be shoveled into carts, and the man who did the shoveling would not trouble to lift out a rat even when he saw one – there were things that went into the sausage in comparison with which a poisoned rat was a tidbit. \\r', 'There was no place for the men to wash their hands before they ate their dinner, and so they made a practice of washing them in the water that was to be ladled into the sausage. There were the butt-ends of smoked meat, and the scraps of corned beef, and all the odds and ends of the waste of the plants, that would be dumped into old barrels in the cellar and left there. Under the system of rigid economy which the packers enforced, there were some jobs that it only paid to do once in a long time, and among these was the cleaning out of the waste barrels. Every spring they did it; and in the barrels would be dirt and rust and old nails and stale water – and cartload after cartload of it would be taken up and dumped into the hoppers with fresh meat, and sent out to the public\\'s breakfast. Some of it they would make into \"smoked\" sausage – but as the smoking took time, and was therefore expensive, they would call upon their chemistry department, and preserve it with borax and color it with gelatin to make it brown. All of their sausage came out of the same bowl, but when they came to wrap it they would stamp some of it \"special,\" and for this they would charge two cents more a pound.']\n"
     ]
    }
   ],
   "source": [
    "for val in prompts_train.prompt_text.values:\n",
    "    print(val.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   prompt_id        2 non-null      object\n",
      " 1   prompt_question  2 non-null      object\n",
      " 2   prompt_title     2 non-null      object\n",
      " 3   prompt_text      2 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 192.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "prompts_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc123</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def789</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_id prompt_question     prompt_title       prompt_text\n",
       "0    abc123    Summarize...  Example Title 1  Heading\\nText...\n",
       "1    def789    Summarize...  Example Title 2  Heading\\nText..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7165 entries, 0 to 7164\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   student_id  7165 non-null   object \n",
      " 1   prompt_id   7165 non-null   object \n",
      " 2   text        7165 non-null   object \n",
      " 3   content     7165 non-null   float64\n",
      " 4   wording     7165 non-null   float64\n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 280.0+ KB\n"
     ]
    }
   ],
   "source": [
    "summaries_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['814d6b', 'ebad26', '3b9047', '39c16e'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_train.prompt_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0020ae56ffbf</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.548304</td>\n",
       "      <td>0.506755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004e978e639e</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>3.128928</td>\n",
       "      <td>4.231226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005ab0199905</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
       "1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n",
       "2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n",
       "3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n",
       "4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
       "\n",
       "    content   wording  \n",
       "0  0.205683  0.380538  \n",
       "1 -0.548304  0.506755  \n",
       "2  3.128928  4.231226  \n",
       "3 -0.210614 -0.471415  \n",
       "4  3.272894  3.219757  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text\n",
       "0  000000ffffff    abc123  Example text 1\n",
       "1  111111eeeeee    def789  Example text 2\n",
       "2  222222cccccc    abc123  Example text 3\n",
       "3  333333dddddd    def789  Example text 4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy\n",
    "==========\n",
    "\n",
    "1. *base model* : deberta v3, lgbm, lstm, randomforest \n",
    "2. *training feature* : prompt_question, prompt_text (based on prompt id)\n",
    "3. *tokenizer* : autotokenizer\n",
    "4. *divide model for two objective* : content / wording\n",
    "5. *training feature for wording* : spell_wrong_num, spell_wrong_words(fixed version)\n",
    "6. *training feature for content* : prompt_question, prompt_text (based on prompt id)\n",
    "7. *additional idea* : \n",
    "- create diff model for four diff prompt_title\n",
    "> impossible. test data will be diffrent, use groupkfold  \n",
    ">  ~~since each prompt topic is diff and indep, model for specific title seems effective~~\n",
    ">> ~~but wording model should not be seperated.~~\n",
    "- stacking / bagging last pred from diff models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "===========\n",
    "\n",
    "## preprocessor \n",
    "+ collect & create useful feature for wording/content scoring\n",
    "\n",
    "## train & validate function for each model\n",
    "\n",
    "+ assign each embedding & shape calibration differently\n",
    "+ depend on model name\n",
    "+ assign different feature columns for training, depends on target value(content/wording) \n",
    "+ use optuna for further improvement\n",
    "\n",
    "## stacking ensemble\n",
    "\n",
    "+ final layer model : linear regression\n",
    "+ ??\n",
    "\n",
    "## predict function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    model_name=['debertav3base', 'lgbm', 'lstm']\n",
    "    learning_rate=0.001\n",
    "    weight_decay=0.005\n",
    "    hidden_dropout_prob=0.005\n",
    "    attention_probs_dropout_prob=0.005\n",
    "    num_train_epochs=5\n",
    "    n_splits=4\n",
    "    batch_size=30 # maybe need adjustment\n",
    "    save_steps=100\n",
    "    max_length=512 # maybe need adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.speller=Speller(lang='en')\n",
    "        self.spellckecker=SpellChecker()\n",
    "        self.STOP_WORDS=set(stopwords.words('english'))\n",
    "        \n",
    "    def word_overlap_count(self,row):\n",
    "        \"\"\"count the words that are used in both prompt text and summary text.\n",
    "        \n",
    "        If the student use same words appeared in prompt text frequently, it would be minus factor to context score.\"\"\"\n",
    "        def check_is_stop_word(word):\n",
    "            return word in self.STOP_WORDS\n",
    "        \n",
    "        prompt_words = row['prompt_tokens']\n",
    "        summary_words = row['summary_tokens']\n",
    "        \n",
    "        # Filter out stopwords if they are defined\n",
    "        if self.STOP_WORDS:\n",
    "            prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
    "            summary_words = list(filter(check_is_stop_word, summary_words))\n",
    "        \n",
    "        # Calculate the count of overlapping words\n",
    "        return len(set(prompt_words).intersection(set(summary_words)))\n",
    "    \n",
    "    def spell_right(self, text):\n",
    "        wordlist=text.split()\n",
    "        wrong_word_cnt=len(list(self.spellckecker.unknown(wordlist)))\n",
    "        return wrong_word_cnt\n",
    "    \n",
    "    def quotes_count(self,row):\n",
    "        summary=row['text']\n",
    "        text=row['prompt_text']\n",
    "        quotes_from_summary=re.findall(r'\"([^\"]*)\"',summary)\n",
    "        if len(quotes_from_summary)>0:\n",
    "            return [quote in text for quote in quotes_from_summary].count(True)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "\n",
    "    def add_train_worddict(self, tokens):\n",
    "        self.spellckecker.word_frequency.load_words(tokens)\n",
    "        self.speller.nlp_data.update({token:1000 for token in tokens})\n",
    "        \n",
    "    def run(self, prompt_df, summary_df):\n",
    "        prompt_df['prompt_length']=prompt_df['prompt_text'].progress_apply(lambda x: len(word_tokenize(x)))\n",
    "        print('-----------prompt length col added!-----------')\n",
    "        prompt_df['prompt_tokens']=prompt_df['prompt_text'].progress_apply(lambda x: word_tokenize(x))\n",
    "        print('-----------prompt tokens col added!-----------')\n",
    "        prompt_df['prompt_tokens'].progress_apply(lambda x: self.add_train_worddict(x))\n",
    "        print('-----------prompt tokens added to worddict!-----------')\n",
    "        print(prompt_df.columns)\n",
    "        \n",
    "        summary_df['summary_length']=summary_df['text'].progress_apply(lambda x: len(word_tokenize(x)))\n",
    "        print('-----------summary length col added!-----------')\n",
    "        summary_df['summary_tokens']=summary_df['text'].progress_apply(lambda x: word_tokenize(x))        \n",
    "        print('-----------summary tokens col added!-----------')\n",
    "        summary_df['wrong_word_cnt']=summary_df['text'].progress_apply(self.spell_right)\n",
    "        print('-----------wrong_word_cnt col added!-----------')\n",
    "        summary_df['fixed_summary_text']=summary_df['text'].progress_apply(self.speller)\n",
    "        print('-----------fixed_summary_text col added!-----------')\n",
    "        print(summary_df.columns)\n",
    "        \n",
    "        input_df=pd.merge(prompt_df, summary_df, on='prompt_id')\n",
    "        print(input_df.columns)\n",
    "        \n",
    "        input_df['word_overlap_cnt']=input_df.progress_apply(self.word_overlap_count, axis=1)\n",
    "        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n",
    "        \n",
    "        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 192.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------prompt length col added!-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 363.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------prompt tokens col added!-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 74.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------prompt tokens added to worddict!-----------\n",
      "Index(['prompt_id', 'prompt_question', 'prompt_title', 'prompt_text',\n",
      "       'prompt_length', 'prompt_tokens'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [00:02<00:00, 3169.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------summary length col added!-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [00:02<00:00, 3179.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------summary tokens col added!-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [00:00<00:00, 10383.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------wrong_word_cnt col added!-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [04:45<00:00, 25.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------fixed_summary_text col added!-----------\n",
      "Index(['student_id', 'prompt_id', 'text', 'content', 'wording',\n",
      "       'summary_length', 'summary_tokens', 'wrong_word_cnt',\n",
      "       'fixed_summary_text'],\n",
      "      dtype='object')\n",
      "Index(['prompt_id', 'prompt_question', 'prompt_title', 'prompt_text',\n",
      "       'prompt_length', 'prompt_tokens', 'student_id', 'text', 'content',\n",
      "       'wording', 'summary_length', 'summary_tokens', 'wrong_word_cnt',\n",
      "       'fixed_summary_text'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [00:00<00:00, 13775.78it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 127920.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------prompt length col added!-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------prompt tokens col added!-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 68.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------prompt tokens added to worddict!-----------\n",
      "Index(['prompt_id', 'prompt_question', 'prompt_title', 'prompt_text',\n",
      "       'prompt_length', 'prompt_tokens'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 3998.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------summary length col added!-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 3997.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------summary tokens col added!-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------wrong_word_cnt col added!-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 3997.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------fixed_summary_text col added!-----------\n",
      "Index(['student_id', 'prompt_id', 'text', 'summary_length', 'summary_tokens',\n",
      "       'wrong_word_cnt', 'fixed_summary_text'],\n",
      "      dtype='object')\n",
      "Index(['prompt_id', 'prompt_question', 'prompt_title', 'prompt_text',\n",
      "       'prompt_length', 'prompt_tokens', 'student_id', 'text',\n",
      "       'summary_length', 'summary_tokens', 'wrong_word_cnt',\n",
      "       'fixed_summary_text'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 4000.29it/s]\n",
      "100%|██████████| 4/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessor=Preprocessor()\n",
    "\n",
    "train=preprocessor.run(prompts_train, summaries_train)\n",
    "print(sizeof_fmt(sys.getsizeof(train)))\n",
    "test=preprocessor.run(prompts_test, summaries_test)\n",
    "print(sizeof_fmt(sys.getsizeof(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "\n",
    "# Iterate through the splits and assign fold numbers to validation data\n",
    "for i, (_, val_index) in enumerate(gkf.split(train, groups=train[\"prompt_id\"])):\n",
    "    train.loc[val_index, \"fold\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>student_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>wrong_word_cnt</th>\n",
       "      <th>fixed_summary_text</th>\n",
       "      <th>word_overlap_cnt</th>\n",
       "      <th>quotes_count</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>699</td>\n",
       "      <td>00791789cc1f</td>\n",
       "      <td>1 element of an ideal tragedy is that it shoul...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>1 element of an ideal tragedy is that it shoul...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>699</td>\n",
       "      <td>0086ef22de8f</td>\n",
       "      <td>The three elements of an ideal tragedy are:  H...</td>\n",
       "      <td>-0.970237</td>\n",
       "      <td>-0.417058</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>The three elements of an ideal tragedy are:  H...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>699</td>\n",
       "      <td>0094589c7a22</td>\n",
       "      <td>Aristotle states that an ideal tragedy should ...</td>\n",
       "      <td>-0.387791</td>\n",
       "      <td>-0.584181</td>\n",
       "      <td>74</td>\n",
       "      <td>17</td>\n",
       "      <td>Aristotle states that an ideal tragedy should ...</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>699</td>\n",
       "      <td>00cd5736026a</td>\n",
       "      <td>One element of an Ideal tragedy is having a co...</td>\n",
       "      <td>0.088882</td>\n",
       "      <td>-0.594710</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>One element of an Ideal tragedy is having a co...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>699</td>\n",
       "      <td>00d98b8ff756</td>\n",
       "      <td>The 3 ideal of tragedy is how complex you need...</td>\n",
       "      <td>-0.687288</td>\n",
       "      <td>-0.460886</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>The 3 ideal of tragedy is how complex you need...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_id                                    prompt_question prompt_title  \\\n",
       "0    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n",
       "1    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n",
       "2    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n",
       "3    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n",
       "4    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n",
       "\n",
       "                                         prompt_text  prompt_length  \\\n",
       "0  Chapter 13 \\r\\nAs the sequel to what has alrea...            699   \n",
       "1  Chapter 13 \\r\\nAs the sequel to what has alrea...            699   \n",
       "2  Chapter 13 \\r\\nAs the sequel to what has alrea...            699   \n",
       "3  Chapter 13 \\r\\nAs the sequel to what has alrea...            699   \n",
       "4  Chapter 13 \\r\\nAs the sequel to what has alrea...            699   \n",
       "\n",
       "     student_id                                               text   content  \\\n",
       "0  00791789cc1f  1 element of an ideal tragedy is that it shoul... -0.210614   \n",
       "1  0086ef22de8f  The three elements of an ideal tragedy are:  H... -0.970237   \n",
       "2  0094589c7a22  Aristotle states that an ideal tragedy should ... -0.387791   \n",
       "3  00cd5736026a  One element of an Ideal tragedy is having a co...  0.088882   \n",
       "4  00d98b8ff756  The 3 ideal of tragedy is how complex you need... -0.687288   \n",
       "\n",
       "    wording  summary_length  wrong_word_cnt  \\\n",
       "0 -0.471415              59               3   \n",
       "1 -0.417058              30               4   \n",
       "2 -0.584181              74              17   \n",
       "3 -0.594710              61               2   \n",
       "4 -0.460886              63               9   \n",
       "\n",
       "                                  fixed_summary_text  word_overlap_cnt  \\\n",
       "0  1 element of an ideal tragedy is that it shoul...                12   \n",
       "1  The three elements of an ideal tragedy are:  H...                10   \n",
       "2  Aristotle states that an ideal tragedy should ...                13   \n",
       "3  One element of an Ideal tragedy is having a co...                13   \n",
       "4  The 3 ideal of tragedy is how complex you need...                11   \n",
       "\n",
       "   quotes_count  fold  \n",
       "0             0   0.0  \n",
       "1             0   0.0  \n",
       "2             4   0.0  \n",
       "3             0   0.0  \n",
       "4             1   0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt_id', 'prompt_question', 'prompt_title', 'prompt_text',\n",
       "       'prompt_length', 'student_id', 'text', 'content', 'wording',\n",
       "       'summary_length', 'wrong_word_cnt', 'fixed_summary_text',\n",
       "       'word_overlap_cnt', 'quotes_count', 'fold'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_columns=['summary_length', 'word_overlap_cnt', 'quotes_count']\n",
    "wording_columns=['wrong_word_cnt', 'word_overlap_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    \n",
    "    # Return RMSE as a dictionary\n",
    "    return {\"rmse\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebertaReg:\n",
    "    def __init__(self, model_name, target_cols, max_length, model_dir,\n",
    "                 hidden_dropout_prob, attention_probs_dropout_prob,\n",
    "                 ):\n",
    "        self.input_col='train_text'\n",
    "        self.model_name=model_name\n",
    "        self.model_dir=model_dir\n",
    "        self.target_cols=target_cols\n",
    "        self.max_length=cfg.max_length\n",
    "        \n",
    "        self.tokenizer=AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "        self.model_config=AutoConfig.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "        \n",
    "        \n",
    "        self.model_config.update({\n",
    "            \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "            \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "        })\n",
    "\n",
    "        self.data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        \n",
    "    def embedding_train(self, train_df):\n",
    "        labels=[train_df['content'], train_df['wording']]\n",
    "        tokenized=self.tokenizer(train_df[self.input_col],\n",
    "                                    padding=\"longest\",\n",
    "                                    truncation=True,\n",
    "                                    max_length=self.max_length)\n",
    "        \n",
    "        return {\n",
    "        **tokenized,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "            \n",
    "    def embedding_test(self, test_df):\n",
    "        labels=[test_df['content'], test_df['wording']]\n",
    "        tokenized=self.tokenizer(test_df[self.input_col],\n",
    "                                    padding=\"longest\",\n",
    "                                    truncation=True,\n",
    "                                    max_length=self.max_length)\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    \n",
    "    def train(self, \n",
    "    fold: int,\n",
    "    train_df: pd.DataFrame,\n",
    "    valid_df: pd.DataFrame,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    weight_decay: float,\n",
    "    num_train_epochs: float,\n",
    "    save_steps: int\n",
    "    ):\n",
    "        \n",
    "        sep=self.tokenizer.sep_token\n",
    "        \n",
    "        train_df[self.input_col] = (\n",
    "                train_df[\"prompt_title\"] + sep \n",
    "                + train_df[\"prompt_question\"] + sep \n",
    "                + train_df[\"fixed_summary_text\"]\n",
    "                )\n",
    "\n",
    "        valid_df[self.input_col] = (\n",
    "                valid_df[\"prompt_title\"] + sep \n",
    "                + valid_df[\"prompt_question\"] + sep \n",
    "                + valid_df[\"fixed_summary_text\"]\n",
    "                )\n",
    "        \n",
    "        train_df=train_df[[self.input_col]+self.target_cols]\n",
    "        valid_df=valid_df[[self.input_col]+self.target_cols]\n",
    "        print(train_df.shape, valid_df.shape)\n",
    "        print(train_df.columns, valid_df.columns)\n",
    "        \n",
    "        model=AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\",\n",
    "                                                                            config=self.model_config)\n",
    "        \n",
    "        train_df=Dataset.from_pandas(train_df, preserve_index=False)\n",
    "        valid_df=Dataset.from_pandas(valid_df, preserve_index=False)\n",
    "        \n",
    "        train_df=train_df.map(self.embedding_train, batched=False)\n",
    "        valid_df=valid_df.map(self.embedding_train, batched=False)\n",
    "        \n",
    "        print(\"----embedding complete----\")\n",
    "        print(sizeof_fmt(sys.getsizeof(train_df)))\n",
    "        print(sizeof_fmt(sys.getsizeof(valid_df)))\n",
    "        \n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_fold_dir,\n",
    "            load_best_model_at_end=True,\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            weight_decay=weight_decay,\n",
    "            report_to='none',\n",
    "            greater_is_better=False,\n",
    "            save_strategy=\"steps\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=save_steps,\n",
    "            save_steps=save_steps,\n",
    "            metric_for_best_model=\"rmse\",\n",
    "            save_total_limit=1\n",
    "        )\n",
    "\n",
    "        # Create a trainer for model training\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_df,\n",
    "            eval_dataset=valid_df,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            data_collator=self.data_collator\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        print('----trainer complete----')\n",
    "        \n",
    "        # Save the trained model and tokenizer\n",
    "        model.save_pretrained(self.model_dir)\n",
    "        self.tokenizer.save_pretrained(self.model_dir)\n",
    "        \n",
    "        model.cpu()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def predict(self, \n",
    "    test_df: pd.DataFrame,\n",
    "    batch_size: int,\n",
    "    fold: int,\n",
    "    ):\n",
    "\n",
    "        sep = self.tokenizer.sep_token\n",
    "        \n",
    "        # Create input text for test data\n",
    "        in_text = (\n",
    "                    test_df[\"prompt_title\"] + sep \n",
    "                    + test_df[\"prompt_question\"] + sep \n",
    "                    + test_df[\"fixed_summary_text\"]\n",
    "                )\n",
    "        test_df[self.input_col] = in_text\n",
    "\n",
    "        # Select the relevant columns\n",
    "        test_df = test_df[[self.input_col]]\n",
    "    \n",
    "        # Create a dataset from the test data\n",
    "        test_df = Dataset.from_pandas(test_df, preserve_index=False) \n",
    "        test_df = test_df.map(self.embedding_test, batched=False)\n",
    "\n",
    "        # Load the trained content score prediction model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "        model.eval()\n",
    "        \n",
    "        # Define model prediction arguments\n",
    "        model_fold_dir = os.path.join(self.model_dir, str(fold)) \n",
    "        \n",
    "        test_args = TrainingArguments(\n",
    "            output_dir=model_fold_dir,\n",
    "            do_train = False,\n",
    "            do_predict = True,\n",
    "            per_device_eval_batch_size =batch_size,\n",
    "            dataloader_drop_last = False,\n",
    "        )\n",
    "\n",
    "        # Initialize a trainer for inference\n",
    "        infer_content = Trainer(\n",
    "                    model = model, \n",
    "                    tokenizer=self.tokenizer,\n",
    "                    data_collator=self.data_collator,\n",
    "                    args = test_args)\n",
    "\n",
    "        # Perform predictions\n",
    "        preds = infer_content.predict(test_df)[0]\n",
    "        pred_df=pd.DataFrame(preds, columns=[\"content_pred\", \"wording_pred\"]\n",
    "        )\n",
    "        \n",
    "        model.cpu()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return pred_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def train_by_fold(\n",
    "        train_df: pd.DataFrame,\n",
    "        model_name: str,\n",
    "        targets: str,\n",
    "        save_each_model: bool,\n",
    "        n_splits: int,\n",
    "        batch_size: int,\n",
    "        learning_rate: int,\n",
    "        hidden_dropout_prob: float,\n",
    "        attention_probs_dropout_prob: float,\n",
    "        weight_decay: float,\n",
    "        num_train_epochs: int,\n",
    "        save_steps: int,\n",
    "        max_length: int\n",
    "    ):\n",
    "\n",
    "    # Delete old model files\n",
    "    if os.path.exists(model_name):\n",
    "        shutil.rmtree(model_name)\n",
    "    \n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "    for fold in range(n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        train_data = train_df[train_df[\"fold\"] != fold]\n",
    "        valid_data = train_df[train_df[\"fold\"] == fold]\n",
    "\n",
    "        model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "\n",
    "        csr = DebertaReg(\n",
    "            model_name=model_name,\n",
    "            target_cols=targets,\n",
    "            model_dir=model_dir, \n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        \n",
    "        csr.train(\n",
    "            fold=fold,\n",
    "            train_df=train_data,\n",
    "            valid_df=valid_data, \n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_steps=save_steps,\n",
    "        )\n",
    "\n",
    "# Define a function for validation (predicting oof data)\n",
    "def validate(\n",
    "    train_df: pd.DataFrame,\n",
    "    targets: str,\n",
    "    save_each_model: bool,\n",
    "    model_name: str,\n",
    "    hidden_dropout_prob: float,\n",
    "    attention_probs_dropout_prob: float,\n",
    "    max_length: int,\n",
    "    batch_size: int\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"Predict out-of-fold (oof) data\"\"\"\n",
    "    for fold in range(cfg.n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        valid_data = train_df[train_df[\"fold\"] == fold]\n",
    "        \n",
    "        model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "        \n",
    "        csr = DebertaReg(\n",
    "            model_name=model_name,\n",
    "            target_cols=targets,\n",
    "            model_dir=model_dir,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        \n",
    "        pred_df = csr.predict(\n",
    "            test_df=valid_data, \n",
    "            batch_size=batch_size,\n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        train_df.loc[valid_data.index, \"content_pred\"] = pred_df[\"content_pred\"].values\n",
    "        train_df.loc[valid_data.index, \"wording_pred\"] = pred_df[\"wording_pred\"].values\n",
    "\n",
    "    return train_df\n",
    "    \n",
    "# Define a function for prediction (using mean folds)\n",
    "def predict(\n",
    "    test_df: pd.DataFrame,\n",
    "    targets: str,\n",
    "    save_each_model: bool,\n",
    "    model_name: str,\n",
    "    hidden_dropout_prob: float,\n",
    "    attention_probs_dropout_prob: float,\n",
    "    max_length: int,\n",
    "    batch_size: int\n",
    "    ):\n",
    "    \"\"\"Predict using mean of folds\"\"\"\n",
    "\n",
    "    for fold in range(cfg.n_splits):\n",
    "        print(f\"fold {fold}:\")\n",
    "        \n",
    "        model_dir =  f\"{model_name}/fold_{fold}\"\n",
    "\n",
    "        csr = DebertaReg(\n",
    "            model_name=model_name,\n",
    "            target_cols=targets,\n",
    "            model_dir=model_dir, \n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        \n",
    "        pred_df = csr.predict(\n",
    "            test_df=test_df, \n",
    "            batch_size=batch_size,\n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        test_df[f\"content_pred_{fold}\"] = pred_df['content_pred'].values\n",
    "        test_df[f\"wording_pred_{fold}\"] = pred_df['wording_pred'].values\n",
    "    \n",
    "    test_df[\"content_pred\"] = test_df[[f\"content_pred_{fold}\" for fold in range(cfg.n_splits)]].mean(axis=1)\n",
    "    test_df[\"wording_pred\"] = test_df[[f\"wording_pred_{fold}\" for fold in range(cfg.n_splits)]].mean(axis=1)\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0:\n",
      "(5108, 3) (2057, 3)\n",
      "Index(['train_text', 'wording', 'content'], dtype='object') Index(['train_text', 'wording', 'content'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 5108/5108 [00:02<00:00, 1822.60 examples/s]\n",
      "Map: 100%|██████████| 2057/2057 [00:00<00:00, 2249.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----embedding complete----\n",
      "48.0 B\n",
      "48.0 B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/855 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 8.00 GiB total capacity; 13.59 GiB already allocated; 0 bytes free; 13.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MINSEOK\\Desktop\\대학생활\\대외활동\\kaggle\\commonlit-evaluate-student-summaries\\student_summaries_lstm_gru_pytorch.ipynb 셀 29\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m t \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mwording\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_by_fold(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdebertav3\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     save_each_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     targets\u001b[39m=\u001b[39;49mt,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mlearning_rate,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     hidden_dropout_prob\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mhidden_dropout_prob,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     attention_probs_dropout_prob\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mattention_probs_dropout_prob,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mweight_decay,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mnum_train_epochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     n_splits\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mn_splits,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     save_steps\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49msave_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mmax_length\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m train \u001b[39m=\u001b[39m validate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     targets\u001b[39m=\u001b[39mt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     max_length\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mmax_length\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mwording\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "\u001b[1;32mc:\\Users\\MINSEOK\\Desktop\\대학생활\\대외활동\\kaggle\\commonlit-evaluate-student-summaries\\student_summaries_lstm_gru_pytorch.ipynb 셀 29\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m model_dir \u001b[39m=\u001b[39m  \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m/fold_\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m csr \u001b[39m=\u001b[39m DebertaReg(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     model_name\u001b[39m=\u001b[39mmodel_name,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     target_cols\u001b[39m=\u001b[39mtargets,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m csr\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     fold\u001b[39m=\u001b[39;49mfold,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     train_df\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     valid_df\u001b[39m=\u001b[39;49mvalid_data, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49mnum_train_epochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     save_steps\u001b[39m=\u001b[39;49msave_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\MINSEOK\\Desktop\\대학생활\\대외활동\\kaggle\\commonlit-evaluate-student-summaries\\student_summaries_lstm_gru_pytorch.ipynb 셀 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m     data_collator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_collator\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m----trainer complete----\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/MINSEOK/Desktop/%EB%8C%80%ED%95%99%EC%83%9D%ED%99%9C/%EB%8C%80%EC%99%B8%ED%99%9C%EB%8F%99/kaggle/commonlit-evaluate-student-summaries/student_summaries_lstm_gru_pytorch.ipynb#X42sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m \u001b[39m# Save the trained model and tokenizer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1596\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1898\u001b[0m ):\n\u001b[0;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:2776\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2773\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2775\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2776\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2779\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:2801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2799\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2800\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2801\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2802\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2803\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2804\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1314\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1314\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeberta(\n\u001b[0;32m   1315\u001b[0m     input_ids,\n\u001b[0;32m   1316\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1317\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1318\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1319\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1320\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1321\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1322\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1323\u001b[0m )\n\u001b[0;32m   1325\u001b[0m encoder_layer \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1326\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1084\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m   1076\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1077\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1078\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1081\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[0;32m   1082\u001b[0m )\n\u001b[1;32m-> 1084\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1085\u001b[0m     embedding_output,\n\u001b[0;32m   1086\u001b[0m     attention_mask,\n\u001b[0;32m   1087\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1088\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1090\u001b[0m )\n\u001b[0;32m   1091\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1093\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:520\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    511\u001b[0m     output_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    512\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    513\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m         rel_embeddings,\n\u001b[0;32m    518\u001b[0m     )\n\u001b[0;32m    519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 520\u001b[0m     output_states \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    521\u001b[0m         next_kv,\n\u001b[0;32m    522\u001b[0m         attention_mask,\n\u001b[0;32m    523\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[0;32m    524\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[0;32m    525\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[0;32m    526\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    527\u001b[0m     )\n\u001b[0;32m    529\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    530\u001b[0m     output_states, att_m \u001b[39m=\u001b[39m output_states\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:362\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    354\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    355\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m ):\n\u001b[1;32m--> 362\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    363\u001b[0m         hidden_states,\n\u001b[0;32m    364\u001b[0m         attention_mask,\n\u001b[0;32m    365\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    366\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[0;32m    367\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[0;32m    368\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    370\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    371\u001b[0m         attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:293\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    285\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    286\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    292\u001b[0m ):\n\u001b[1;32m--> 293\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    294\u001b[0m         hidden_states,\n\u001b[0;32m    295\u001b[0m         attention_mask,\n\u001b[0;32m    296\u001b[0m         output_attentions,\n\u001b[0;32m    297\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[0;32m    298\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[0;32m    299\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    302\u001b[0m         self_output, att_matrix \u001b[39m=\u001b[39m self_output\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:740\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[39m# bsz x height x length x dimension\u001b[39;00m\n\u001b[0;32m    739\u001b[0m attention_probs \u001b[39m=\u001b[39m XSoftmax\u001b[39m.\u001b[39mapply(attention_scores, attention_mask, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 740\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(attention_probs)\n\u001b[0;32m    741\u001b[0m context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(\n\u001b[0;32m    742\u001b[0m     attention_probs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, attention_probs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m), attention_probs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), value_layer\n\u001b[0;32m    743\u001b[0m )\n\u001b[0;32m    744\u001b[0m context_layer \u001b[39m=\u001b[39m (\n\u001b[0;32m    745\u001b[0m     context_layer\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, context_layer\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m), context_layer\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m    746\u001b[0m     \u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m    747\u001b[0m     \u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    748\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:234\u001b[0m, in \u001b[0;36mStableDropout.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39mCall the module\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \n\u001b[0;32m    230\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39m    x (`torch.tensor`): The input tensor to apply dropout\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_prob \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m XDropout\u001b[39m.\u001b[39;49mapply(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_context())\n\u001b[0;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mapply(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[0;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MINSEOK\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:181\u001b[0m, in \u001b[0;36mXDropout.forward\u001b[1;34m(ctx, input, local_ctx)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m dropout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    180\u001b[0m     ctx\u001b[39m.\u001b[39msave_for_backward(mask)\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mmasked_fill(mask, \u001b[39m0\u001b[39;49m) \u001b[39m*\u001b[39;49m ctx\u001b[39m.\u001b[39;49mscale\n\u001b[0;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 8.00 GiB total capacity; 13.59 GiB already allocated; 0 bytes free; 13.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "t = [\"wording\", \"content\"]\n",
    "    \n",
    "train_by_fold(\n",
    "    train,\n",
    "    model_name=\"debertav3\",\n",
    "    save_each_model=False,\n",
    "    targets=t,\n",
    "    learning_rate=cfg.learning_rate,\n",
    "    hidden_dropout_prob=cfg.hidden_dropout_prob,\n",
    "    attention_probs_dropout_prob=cfg.attention_probs_dropout_prob,\n",
    "    weight_decay=cfg.weight_decay,\n",
    "    num_train_epochs=cfg.num_train_epochs,\n",
    "    n_splits=cfg.n_splits,\n",
    "    batch_size=cfg.batch_size,\n",
    "    save_steps=cfg.save_steps,\n",
    "    max_length=cfg.max_length\n",
    ")\n",
    "\n",
    "\n",
    "train = validate(\n",
    "    train,\n",
    "    targets=t,\n",
    "    save_each_model=False,\n",
    "    model_name=cfg.model_name,\n",
    "    hidden_dropout_prob=cfg.hidden_dropout_prob,\n",
    "    attention_probs_dropout_prob=cfg.attention_probs_dropout_prob,\n",
    "    max_length=cfg.max_length\n",
    ")\n",
    "\n",
    "for target in [\"content\",\"wording\"]:\n",
    "    rmse = mean_squared_error(train[target], train[f\"{target}_pred\"], squared=False)\n",
    "    print(f\"cv {target} rmse: {rmse}\")\n",
    "\n",
    "test = predict(\n",
    "    test,\n",
    "    targets=t,\n",
    "    save_each_model=False,\n",
    "    model_name=cfg.model_name,\n",
    "    hidden_dropout_prob=cfg.hidden_dropout_prob,\n",
    "    attention_probs_dropout_prob=cfg.attention_probs_dropout_prob,\n",
    "    max_length=cfg.max_length\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vars = list(globals().items())\n",
    "variables = [(var, (sys.getsizeof(obj))) for var, obj in local_vars]\n",
    "variables = sorted(((var, size_value) for var, size_value in variables), key=lambda x: -x[1])\n",
    "variables = [(var, sizeof_fmt(size_value)) for var, size_value in variables]\n",
    "\n",
    "for var, size_fmt in variables:\n",
    "    print(\" {:>30}: {:>8}\".format(var, size_fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBMReg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMReg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "wording_feature=['wrong_word_cnt','word_overlap_cnt']\n",
    "\n",
    "def objective(trial,model_name, target):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if model_name=='debertav3base':\n",
    "        and\n",
    "    elif model_name=='lgbmregressor':\n",
    "        and\n",
    "    elif modle_name=='lstm':\n",
    "        and"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
